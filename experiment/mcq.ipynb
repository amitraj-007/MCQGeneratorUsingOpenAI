{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(openai_api_key=key,model_name=\"gpt-3.5-turbo\",temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001AEED9EEB10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AEEDA009B0>, root_client=<openai.OpenAI object at 0x000001AEED78F8F0>, root_async_client=<openai.AsyncOpenAI object at 0x000001AEED9EEB70>, temperature=0.5, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be confirming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\",\"number\",\"subject\",\"tone\",\"response_json\"],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain=LLMChain(llm=llm,prompt=quiz_generation_prompt,output_key=\"quiz\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(input_variables=[\"subject\", \"quiz\"], template=TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain=LLMChain(llm=llm, prompt=quiz_evaluation_prompt, output_key=\"review\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain=SequentialChain(chains=[quiz_chain, review_chain], input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "                                        output_variables=[\"quiz\", \"review\"], verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"D:\\mcqgenerator\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\mcqgenerator\\\\data.txt'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is LangChain?\n",
      "LangChain is a framework designed to build applications powered by Large Language Models (LLMs), such as OpenAI's GPT-3.5 or GPT-4. It provides tools, abstractions, and integrations that make it easier to create applications like chatbots, question-answering systems, summarization tools, and more.\n",
      "\n",
      "LangChain's core philosophy is modularity and chainability, meaning it helps developers break down complex workflows into smaller, manageable, and reusable components.\n",
      "\n",
      "Key Components of LangChain\n",
      "LLMs (Large Language Models):\n",
      "\n",
      "LangChain supports multiple LLMs (e.g., OpenAI, Hugging Face, Cohere).\n",
      "It provides wrappers to interact with these models and configure parameters like temperature, model_name, and max_tokens.\n",
      "Prompts:\n",
      "\n",
      "LangChain offers tools to manage, format, and template prompts effectively.\n",
      "You can define reusable prompt templates and dynamically inject variables.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "template = \"Translate the following text to French: {text}\"\n",
      "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
      "Chains:\n",
      "\n",
      "Chains allow you to link multiple steps in a pipeline.\n",
      "For example, a chain might take a userâ€™s query, call an LLM to process it, and then store the result in a database.\n",
      "Types of Chains:\n",
      "\n",
      "Simple Chains: Sequentially process inputs and outputs.\n",
      "Sequential Chains: Combine multiple chains in sequence.\n",
      "Memory Chains: Keep context across multiple calls, useful for chatbots.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "\n",
      "llm = ChatOpenAI(temperature=0.7)\n",
      "prompt = PromptTemplate(template=\"What's the capital of {country}?\", input_variables=[\"country\"])\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "result = chain.run({\"country\": \"France\"})\n",
      "print(result)  # \"Paris\"\n",
      "Memory:\n",
      "\n",
      "Memory is used to store the state of a conversation or process.\n",
      "Example: Keeping track of previous questions and answers in a chatbot.\n",
      "Types:\n",
      "\n",
      "Short-Term Memory: Used for recent interactions.\n",
      "Long-Term Memory: Persistent memory for long-term context.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "memory = ConversationBufferMemory()\n",
      "Agents:\n",
      "\n",
      "Agents dynamically decide which action to take based on user input and available tools.\n",
      "Example: A chatbot that can perform calculations, retrieve documents, or generate text.\n",
      "Example Agent Workflow:\n",
      "\n",
      "Input: \"What's 2+2?\"\n",
      "Agent decides to call a calculator tool.\n",
      "Output: \"4\"\n",
      "Tools:\n",
      "\n",
      "Tools are external utilities that the LLM can use.\n",
      "Examples:\n",
      "Calculators\n",
      "Search engines\n",
      "Custom APIs\n",
      "Tools can be combined with agents to build dynamic systems.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.agents import load_tools\n",
      "tools = load_tools([\"serpapi\", \"llm-math\"])\n",
      "Retrievers and Vector Stores:\n",
      "\n",
      "LangChain supports integration with vector databases (e.g., Pinecone, Weaviate, FAISS) to retrieve relevant documents.\n",
      "Useful for building question-answering systems with large datasets.\n",
      "Why Use LangChain?\n",
      "Modularity:\n",
      "\n",
      "Break your application into manageable components.\n",
      "Reuse components across multiple projects.\n",
      "Ease of Integration:\n",
      "\n",
      "LangChain integrates with popular LLM providers, databases, APIs, and tools.\n",
      "Support for Complex Workflows:\n",
      "\n",
      "Build applications that involve multiple steps, dynamic decisions, or memory.\n",
      "Built-in Memory:\n",
      "\n",
      "Enables conversational agents to retain context.\n",
      "Rich Ecosystem:\n",
      "\n",
      "Plug-and-play support for many tools like search engines, calculators, APIs, and more.\n",
      "LangChain Ecosystem\n",
      "LLM Providers:\n",
      "\n",
      "OpenAI (e.g., GPT-3.5, GPT-4)\n",
      "Hugging Face models\n",
      "Cohere, Anthropic, and more.\n",
      "Vector Databases:\n",
      "\n",
      "Pinecone, Weaviate, FAISS, and Chroma for document retrieval and embeddings.\n",
      "Tool Integration:\n",
      "\n",
      "Google Search (via SerpAPI)\n",
      "Python REPL\n",
      "WolframAlpha\n",
      "Custom APIs.\n",
      "Frameworks:\n",
      "\n",
      "Works well with Python, but also supports JavaScript/TypeScript.\n",
      "Common Use Cases\n",
      "Chatbots:\n",
      "\n",
      "Build conversational agents with memory and tool integrations.\n",
      "Document QA:\n",
      "\n",
      "Answer questions from documents by retrieving and summarizing content.\n",
      "Summarization:\n",
      "\n",
      "Summarize long texts or documents into concise formats.\n",
      "Code Assistance:\n",
      "\n",
      "Use agents to write, debug, or analyze code.\n",
      "Custom Workflows:\n",
      "\n",
      "Build domain-specific applications like legal advisors, customer support bots, or creative writing assistants.\n",
      "Installation\n",
      "To get started with LangChain:\n",
      "\n",
      "bash\n",
      "Copy\n",
      "Edit\n",
      "pip install langchain\n",
      "pip install openai  # For OpenAI integrations\n",
      "Basic Example\n",
      "Hereâ€™s a simple chatbot example using LangChain:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.chains import ConversationChain\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
      "memory = ConversationBufferMemory()\n",
      "\n",
      "chatbot = ConversationChain(llm=llm, memory=memory)\n",
      "\n",
      "response = chatbot.run(\"Hello, how can I assist you?\")\n",
      "print(response)\n"
     ]
    }
   ],
   "source": [
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize the Python dictionary into a JSON-formatted string\n",
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER=5 \n",
    "SUBJECT=\"LangChain\"\n",
    "TONE=\"hard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:What is LangChain?\n",
      "LangChain is a framework designed to build applications powered by Large Language Models (LLMs), such as OpenAI's GPT-3.5 or GPT-4. It provides tools, abstractions, and integrations that make it easier to create applications like chatbots, question-answering systems, summarization tools, and more.\n",
      "\n",
      "LangChain's core philosophy is modularity and chainability, meaning it helps developers break down complex workflows into smaller, manageable, and reusable components.\n",
      "\n",
      "Key Components of LangChain\n",
      "LLMs (Large Language Models):\n",
      "\n",
      "LangChain supports multiple LLMs (e.g., OpenAI, Hugging Face, Cohere).\n",
      "It provides wrappers to interact with these models and configure parameters like temperature, model_name, and max_tokens.\n",
      "Prompts:\n",
      "\n",
      "LangChain offers tools to manage, format, and template prompts effectively.\n",
      "You can define reusable prompt templates and dynamically inject variables.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "template = \"Translate the following text to French: {text}\"\n",
      "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
      "Chains:\n",
      "\n",
      "Chains allow you to link multiple steps in a pipeline.\n",
      "For example, a chain might take a userâ€™s query, call an LLM to process it, and then store the result in a database.\n",
      "Types of Chains:\n",
      "\n",
      "Simple Chains: Sequentially process inputs and outputs.\n",
      "Sequential Chains: Combine multiple chains in sequence.\n",
      "Memory Chains: Keep context across multiple calls, useful for chatbots.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "\n",
      "llm = ChatOpenAI(temperature=0.7)\n",
      "prompt = PromptTemplate(template=\"What's the capital of {country}?\", input_variables=[\"country\"])\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "result = chain.run({\"country\": \"France\"})\n",
      "print(result)  # \"Paris\"\n",
      "Memory:\n",
      "\n",
      "Memory is used to store the state of a conversation or process.\n",
      "Example: Keeping track of previous questions and answers in a chatbot.\n",
      "Types:\n",
      "\n",
      "Short-Term Memory: Used for recent interactions.\n",
      "Long-Term Memory: Persistent memory for long-term context.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "memory = ConversationBufferMemory()\n",
      "Agents:\n",
      "\n",
      "Agents dynamically decide which action to take based on user input and available tools.\n",
      "Example: A chatbot that can perform calculations, retrieve documents, or generate text.\n",
      "Example Agent Workflow:\n",
      "\n",
      "Input: \"What's 2+2?\"\n",
      "Agent decides to call a calculator tool.\n",
      "Output: \"4\"\n",
      "Tools:\n",
      "\n",
      "Tools are external utilities that the LLM can use.\n",
      "Examples:\n",
      "Calculators\n",
      "Search engines\n",
      "Custom APIs\n",
      "Tools can be combined with agents to build dynamic systems.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.agents import load_tools\n",
      "tools = load_tools([\"serpapi\", \"llm-math\"])\n",
      "Retrievers and Vector Stores:\n",
      "\n",
      "LangChain supports integration with vector databases (e.g., Pinecone, Weaviate, FAISS) to retrieve relevant documents.\n",
      "Useful for building question-answering systems with large datasets.\n",
      "Why Use LangChain?\n",
      "Modularity:\n",
      "\n",
      "Break your application into manageable components.\n",
      "Reuse components across multiple projects.\n",
      "Ease of Integration:\n",
      "\n",
      "LangChain integrates with popular LLM providers, databases, APIs, and tools.\n",
      "Support for Complex Workflows:\n",
      "\n",
      "Build applications that involve multiple steps, dynamic decisions, or memory.\n",
      "Built-in Memory:\n",
      "\n",
      "Enables conversational agents to retain context.\n",
      "Rich Ecosystem:\n",
      "\n",
      "Plug-and-play support for many tools like search engines, calculators, APIs, and more.\n",
      "LangChain Ecosystem\n",
      "LLM Providers:\n",
      "\n",
      "OpenAI (e.g., GPT-3.5, GPT-4)\n",
      "Hugging Face models\n",
      "Cohere, Anthropic, and more.\n",
      "Vector Databases:\n",
      "\n",
      "Pinecone, Weaviate, FAISS, and Chroma for document retrieval and embeddings.\n",
      "Tool Integration:\n",
      "\n",
      "Google Search (via SerpAPI)\n",
      "Python REPL\n",
      "WolframAlpha\n",
      "Custom APIs.\n",
      "Frameworks:\n",
      "\n",
      "Works well with Python, but also supports JavaScript/TypeScript.\n",
      "Common Use Cases\n",
      "Chatbots:\n",
      "\n",
      "Build conversational agents with memory and tool integrations.\n",
      "Document QA:\n",
      "\n",
      "Answer questions from documents by retrieving and summarizing content.\n",
      "Summarization:\n",
      "\n",
      "Summarize long texts or documents into concise formats.\n",
      "Code Assistance:\n",
      "\n",
      "Use agents to write, debug, or analyze code.\n",
      "Custom Workflows:\n",
      "\n",
      "Build domain-specific applications like legal advisors, customer support bots, or creative writing assistants.\n",
      "Installation\n",
      "To get started with LangChain:\n",
      "\n",
      "bash\n",
      "Copy\n",
      "Edit\n",
      "pip install langchain\n",
      "pip install openai  # For OpenAI integrations\n",
      "Basic Example\n",
      "Hereâ€™s a simple chatbot example using LangChain:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.chains import ConversationChain\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
      "memory = ConversationBufferMemory()\n",
      "\n",
      "chatbot = ConversationChain(llm=llm, memory=memory)\n",
      "\n",
      "response = chatbot.run(\"Hello, how can I assist you?\")\n",
      "print(response)\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 5 multiple choice questions for LangChain students in hard tone. \n",
      "Make sure the questions are not repeated and check all the questions to be confirming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 5 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:What is LangChain?\n",
      "LangChain is a framework designed to build applications powered by Large Language Models (LLMs), such as OpenAI's GPT-3.5 or GPT-4. It provides tools, abstractions, and integrations that make it easier to create applications like chatbots, question-answering systems, summarization tools, and more.\n",
      "\n",
      "LangChain's core philosophy is modularity and chainability, meaning it helps developers break down complex workflows into smaller, manageable, and reusable components.\n",
      "\n",
      "Key Components of LangChain\n",
      "LLMs (Large Language Models):\n",
      "\n",
      "LangChain supports multiple LLMs (e.g., OpenAI, Hugging Face, Cohere).\n",
      "It provides wrappers to interact with these models and configure parameters like temperature, model_name, and max_tokens.\n",
      "Prompts:\n",
      "\n",
      "LangChain offers tools to manage, format, and template prompts effectively.\n",
      "You can define reusable prompt templates and dynamically inject variables.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "template = \"Translate the following text to French: {text}\"\n",
      "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
      "Chains:\n",
      "\n",
      "Chains allow you to link multiple steps in a pipeline.\n",
      "For example, a chain might take a userâ€™s query, call an LLM to process it, and then store the result in a database.\n",
      "Types of Chains:\n",
      "\n",
      "Simple Chains: Sequentially process inputs and outputs.\n",
      "Sequential Chains: Combine multiple chains in sequence.\n",
      "Memory Chains: Keep context across multiple calls, useful for chatbots.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "\n",
      "llm = ChatOpenAI(temperature=0.7)\n",
      "prompt = PromptTemplate(template=\"What's the capital of {country}?\", input_variables=[\"country\"])\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "result = chain.run({\"country\": \"France\"})\n",
      "print(result)  # \"Paris\"\n",
      "Memory:\n",
      "\n",
      "Memory is used to store the state of a conversation or process.\n",
      "Example: Keeping track of previous questions and answers in a chatbot.\n",
      "Types:\n",
      "\n",
      "Short-Term Memory: Used for recent interactions.\n",
      "Long-Term Memory: Persistent memory for long-term context.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "memory = ConversationBufferMemory()\n",
      "Agents:\n",
      "\n",
      "Agents dynamically decide which action to take based on user input and available tools.\n",
      "Example: A chatbot that can perform calculations, retrieve documents, or generate text.\n",
      "Example Agent Workflow:\n",
      "\n",
      "Input: \"What's 2+2?\"\n",
      "Agent decides to call a calculator tool.\n",
      "Output: \"4\"\n",
      "Tools:\n",
      "\n",
      "Tools are external utilities that the LLM can use.\n",
      "Examples:\n",
      "Calculators\n",
      "Search engines\n",
      "Custom APIs\n",
      "Tools can be combined with agents to build dynamic systems.\n",
      "Example:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.agents import load_tools\n",
      "tools = load_tools([\"serpapi\", \"llm-math\"])\n",
      "Retrievers and Vector Stores:\n",
      "\n",
      "LangChain supports integration with vector databases (e.g., Pinecone, Weaviate, FAISS) to retrieve relevant documents.\n",
      "Useful for building question-answering systems with large datasets.\n",
      "Why Use LangChain?\n",
      "Modularity:\n",
      "\n",
      "Break your application into manageable components.\n",
      "Reuse components across multiple projects.\n",
      "Ease of Integration:\n",
      "\n",
      "LangChain integrates with popular LLM providers, databases, APIs, and tools.\n",
      "Support for Complex Workflows:\n",
      "\n",
      "Build applications that involve multiple steps, dynamic decisions, or memory.\n",
      "Built-in Memory:\n",
      "\n",
      "Enables conversational agents to retain context.\n",
      "Rich Ecosystem:\n",
      "\n",
      "Plug-and-play support for many tools like search engines, calculators, APIs, and more.\n",
      "LangChain Ecosystem\n",
      "LLM Providers:\n",
      "\n",
      "OpenAI (e.g., GPT-3.5, GPT-4)\n",
      "Hugging Face models\n",
      "Cohere, Anthropic, and more.\n",
      "Vector Databases:\n",
      "\n",
      "Pinecone, Weaviate, FAISS, and Chroma for document retrieval and embeddings.\n",
      "Tool Integration:\n",
      "\n",
      "Google Search (via SerpAPI)\n",
      "Python REPL\n",
      "WolframAlpha\n",
      "Custom APIs.\n",
      "Frameworks:\n",
      "\n",
      "Works well with Python, but also supports JavaScript/TypeScript.\n",
      "Common Use Cases\n",
      "Chatbots:\n",
      "\n",
      "Build conversational agents with memory and tool integrations.\n",
      "Document QA:\n",
      "\n",
      "Answer questions from documents by retrieving and summarizing content.\n",
      "Summarization:\n",
      "\n",
      "Summarize long texts or documents into concise formats.\n",
      "Code Assistance:\n",
      "\n",
      "Use agents to write, debug, or analyze code.\n",
      "Custom Workflows:\n",
      "\n",
      "Build domain-specific applications like legal advisors, customer support bots, or creative writing assistants.\n",
      "Installation\n",
      "To get started with LangChain:\n",
      "\n",
      "bash\n",
      "Copy\n",
      "Edit\n",
      "pip install langchain\n",
      "pip install openai  # For OpenAI integrations\n",
      "Basic Example\n",
      "Hereâ€™s a simple chatbot example using LangChain:\n",
      "\n",
      "python\n",
      "Copy\n",
      "Edit\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.chains import ConversationChain\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
      "memory = ConversationBufferMemory()\n",
      "\n",
      "chatbot = ConversationChain(llm=llm, memory=memory)\n",
      "\n",
      "response = chatbot.run(\"Hello, how can I assist you?\")\n",
      "print(response)\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 5 multiple choice questions for LangChain students in hard tone. \n",
      "Make sure the questions are not repeated and check all the questions to be confirming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 5 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/modules/model_io/llms/token_usage_tracking\n",
    "\n",
    "#How to setup Token Usage Tracking in LangChain\n",
    "with get_openai_callback() as cb:\n",
    "    response=generate_evaluate_chain(\n",
    "        {\n",
    "            \"text\": TEXT,\n",
    "            \"number\": NUMBER,\n",
    "            \"subject\":SUBJECT,\n",
    "            \"tone\": TONE,\n",
    "            \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "        }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:3408\n",
      "Prompt Tokens:2660\n",
      "Completion Tokens:748\n",
      "Total Cost:0.002452\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Tokens:{cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens:{cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens:{cb.completion_tokens}\")\n",
    "print(f\"Total Cost:{cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": {\n",
      "        \"mcq\": \"What is the core philosophy of LangChain?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Modularity and chainability\",\n",
      "            \"b\": \"Flexibility and scalability\",\n",
      "            \"c\": \"Efficiency and speed\",\n",
      "            \"d\": \"Accuracy and precision\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"mcq\": \"Which component of LangChain is used to store the state of a conversation or process?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"LLMs\",\n",
      "            \"b\": \"Prompts\",\n",
      "            \"c\": \"Memory\",\n",
      "            \"d\": \"Chains\"\n",
      "        },\n",
      "        \"correct\": \"c\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"mcq\": \"What type of memory in LangChain is used for recent interactions?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Short-Term Memory\",\n",
      "            \"b\": \"Long-Term Memory\",\n",
      "            \"c\": \"Sequential Memory\",\n",
      "            \"d\": \"Persistent Memory\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"mcq\": \"Which component of LangChain dynamically decides which action to take based on user input and available tools?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Memory\",\n",
      "            \"b\": \"Agents\",\n",
      "            \"c\": \"Tools\",\n",
      "            \"d\": \"Retrievers\"\n",
      "        },\n",
      "        \"correct\": \"b\"\n",
      "    },\n",
      "    \"5\": {\n",
      "        \"mcq\": \"What is one of the common use cases of LangChain?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Building cars\",\n",
      "            \"b\": \"Cooking recipes\",\n",
      "            \"c\": \"Chatbots\",\n",
      "            \"d\": \"Weather forecasting\"\n",
      "        },\n",
      "        \"correct\": \"c\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "quiz=response.get(\"quiz\")\n",
    "print(quiz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data = []\n",
    "for key, value in quiz.items():\n",
    "    mcq = value[\"mcq\"]\n",
    "    options = \" | \".join(\n",
    "        [\n",
    "            f\"{option}: {option_value}\"\n",
    "            for option, option_value in value[\"options\"].items()\n",
    "            ]\n",
    "        )\n",
    "    correct = value[\"correct\"]\n",
    "    quiz_table_data.append({\"MCQ\": mcq, \"Choices\": options, \"Correct\": correct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MCQ': 'What is the core philosophy of LangChain?',\n",
       "  'Choices': 'a: Modularity and chainability | b: Flexibility and scalability | c: Efficiency and speed | d: Accuracy and precision',\n",
       "  'Correct': 'a'},\n",
       " {'MCQ': 'Which component of LangChain is used to store the state of a conversation or process?',\n",
       "  'Choices': 'a: LLMs | b: Prompts | c: Memory | d: Chains',\n",
       "  'Correct': 'c'},\n",
       " {'MCQ': 'What type of memory in LangChain is used for recent interactions?',\n",
       "  'Choices': 'a: Short-Term Memory | b: Long-Term Memory | c: Sequential Memory | d: Persistent Memory',\n",
       "  'Correct': 'a'},\n",
       " {'MCQ': 'Which component of LangChain dynamically decides which action to take based on user input and available tools?',\n",
       "  'Choices': 'a: Memory | b: Agents | c: Tools | d: Retrievers',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What is one of the common use cases of LangChain?',\n",
       "  'Choices': 'a: Building cars | b: Cooking recipes | c: Chatbots | d: Weather forecasting',\n",
       "  'Correct': 'c'}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz=pd.DataFrame(quiz_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz.to_csv(\"machinelearning.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01_22_2025_19_08_26'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime('%m_%d_%Y_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
